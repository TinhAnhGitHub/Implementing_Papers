# config/trainer_config.yaml

defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# Core Configuration

config_name: "SISR_config_train"
core:
  seed: 3407
  experiment_name: "project_SISR"
  project_dir: "unet_sisr"  
  run_name: "run_${now:%Y%m%d_%H%M%S}"  
  device: "cuda"
  deterministic: False
  precision: "fp32"  # Options: fp16, b16
  enable_benchmarking: True
  

# Data Configuration
data:
  paths:
    train: "data/train"
    val: "data/val"

  loaders:
    num_workers: 4
    pin_memory: true
    persistent_workers: True

  batch_size:
    train: 32
    val: 32

  normalization:
    mean: [0.485, 0.456, 0.406]  
    std: [0.229, 0.224, 0.225]

  preprocessing:
    resize: [1024, 644]
  

# Model Configuration
model:
  architecture: "unet"  # Options: unet, runet
  input_channels: 3
  num_classes: 3
  use_norm: True
  use_act: True
  use_dropout: True
  act_type: "LeakyReLU"  
  norm_type: "BatchNorm2d"  
  up_type: "Upsample"  
  up_mode: "bilinear"
  dropout_probability: 0.1

  runet:
    features_encode: [64, 128, 256, 512]
    features_decode: [512, 512, 384, 256, 96]
    downsample_each_output_layer: [True, True, True, False]
    feature_initial: 64
    feature_near_final: 96
    num_block_each_feature: [4, 4, 6, 2]
    double_last_blocks_each_feature: [False, True, True, True]

  unet:
    features: [64, 128, 256, 512, 1024]
    use_skip: True

# Training Configuration
training:
  num_epochs: 50
  gradient_accumulation_steps: 1
  
  lr_scheduler:
    name: "onecyclelr"
    warmup_epochs: 5
    warmup_momentum: 0.8
    warmup_bias_lr: 0.1
    min_lr: 1e-6
    cycle_momentum: True
  
  loss:
    use_feature_loss: True
    feature_model: "vgg19"  # Options: vgg19, resnet50
    feature_layer: "features.35"


# Optimizer Configuration
optimizer:
  name: "adamw" #adam, adamw, sgd
  lr: 1e-3
  weight_decay: 1e-5
  momentum: 0.9037
  betas: [0.9, 0.999] 
  eps: 1e-8
  grad_clip:
    enable: True
    max_norm: 1.0
    norm_type: 2.0




augmentation:
  color:
    brightness: 0.015
    contrast: 0.7
    saturation: 0.4
    hue: 0.0

  geometric:
    degrees: 0.0
    translate: 0.1
    scale: 0.5
    shear: 0.0
    perspective: 0.0
    flipud_prob: 0.0
    fliplr_prob: 0.5

  regularization:
    erasing_prob: 0.4
    crop_fraction: 1.0


logging:
    refresh_rate: 10
    log_dir: "${core.project_dir}/${core.experiment_name}/${core.run_name}/logs"

  



metrics:
  train: ["loss", "ssim", "psnr"]
  val: ["loss", "ssim", "psnr"]


callbacks:
  modelckpt:
    enable: True
    prefixfilename: "${core.project_dir}/${core.experiment_name}/${core.run_name}/checkpoints/best_model_${model.architecture}"
    save_on_epoch: True
    num_epoch_save: 3
    max_keeps: 3
    keep_condition: "val_ssim"
    mode: "max"

  wandb:
    enabled: True
    project: "my_project"
    dir: "${core.project_dir}/${core.experiment_name}/${core.run_name}/wandb_log"
    id: "v0.1"
    name: "${core.run_name}"
    notes: "Just a pipeline testing"
    tags: ["SISR"]
    log_train_metrics: True 
    log_val_metrics: True 
    log_grad_norm: True

  early_stopping:
    enable: True
    patience: 10
    monitor: "val_loss" 
    mode: "min"

  awp:
    enabled: True
    adv_lr: 1e-4
    adv_eps: 0.001
    adv_param: 'weight'

  ema:
    enabled: True
    decay: 0.999

  swa:
    enabled: True
    swa_start: 10 
    swa_freq: 5
    swa_lr: 0.05 

  timer:
    enable: True


  
  apply_communication_hook_compression:
    enable: True  
    type: "fp16" # bf16, powersgd
  

  visualization:
    enable: True
    output_dir: "${core.project_dir}/${core.experiment_name}/${core.run_name}/visualizations"
    save_examples: True
    num_examples: 5  